---
title:  "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks"
categories: update
---

본 연구에서 집중하는 것은 효율적인 channel attention을 만드는 것이다. 기존 SE-Layer는 두개의 FC-layer를 이용하여 cross channel interaction을 구하게 되는데 dimension reduction을 통해 model complexity를 줄이게 된다. 하지만 실험결과 dimension reduction이 side effect가 있다는 것을 발견했다. dimension reduction을 없애고 1D convolution을 이용해서 efficient하게 channel attention을 하는 방법을 제안했다. 


![그림1](https://user-images.githubusercontent.com/46548053/111124992-7779a380-85b4-11eb-9d08-c2c824b8c636.png)


Squeeze and Excitation Network의 성능을 분석하기 위해서 SE-layer와 SE-Var1~3와의 성능을 비교 분석하였다. Channel Attention만을 사용한 SE-VAR1이 Vanilla 모델보다 성능이 좋았다. 반면 Dimension reduction 없이 각 channel마다 independent하게 weight을 주는 SE-Var2가 SE-Layer보다 성능이 좋았으며 single layer를 통과한 SE-Var3이 two layer에 Dimension reduction을 사용한 SE-layer보다 성능이 좋았다. 이를 통해 Channel Attention은 효과가 있는 반면 Dimension Reduction은 효과가 없다는 사실을 알 수 있다.


![그림2](https://user-images.githubusercontent.com/46548053/111125058-8a8c7380-85b4-11eb-92ae-2a364a6306e2.png)


Local cross channel attention을 kernel size k 인 1D convolution으로 구현했다.   
![그림3](https://user-images.githubusercontent.com/46548053/111125065-8bbda080-85b4-11eb-8811-54656d00f51d.png)  
FC layer를 1D Convolution으로 대체함에 따라 Dimension Reduction 없이 효율적으로 Local cross channel attention을 수행할 수 있다.

실험
---
![그림4](https://user-images.githubusercontent.com/46548053/111125070-8ceecd80-85b4-11eb-90ef-a13279515681.png)


실험 결과를 보면 ResNet, MobileNet backbone에서 ECA-Net은 SENet대비 Parameter 수가 적으면서도 성능은 개선되었다. 


![그림5](https://user-images.githubusercontent.com/46548053/111125078-8eb89100-85b4-11eb-88d5-74c7a8054153.png)


![그림6](https://user-images.githubusercontent.com/46548053/111125083-8f512780-85b4-11eb-8969-0473ae544140.png)


Object Detection, Instance Segmentation 등의 Downstream task에서도 ECA-Net은 SE-Net이나 vanilla Network에 비해 성능이 개선되었다. 
