---
title:  "Learning Transferable Visual Models From Natural Language Supervision"
categories: update
---

Learning Transferable Visual Models From Natural Language Supervision
===

Introduction
---

기존 연구들을 보면 natural language supervision 사용한 image recognition 성능이 좋지 않았다. 예를 들어 Li et al. (2017)의 연구는 zero shot learning에서 겨우 11.5%의 성능을 보여주었다. 반면 좀더 narrow scoped weak supervision의 경우 좋은 결과를 보여주었다. ImageNet과 관련된 Instagram hashtags를 이용한 경우에는 fine-tuning했을 경우 성능이 5% 올랐다.

 
이러한 차이가 발생한 이유는 scale의 문제이다. 기존 natural language supervision 사용한 연구(VirTex, ICMLM, and ConVIRT 등)는 겨우 20~200만개의 image들을 몇일간 training했다. 반면 다른 weak supervision연구에서는 몇억개의 image들을 몇년간 training했다. 이 연구에서는 대규모의 natural language supervision을 이용한 연구를 진행했다. natural language supervision으로 training한 모델을 CLIP(Contrastive Language-Image Pre-training)라고 부르는데 OCR, geo-localization, action recognition등 다양한 task에서 좋은 성능을 보여주었고 CLIP을 zero-shot learning한 것이 ImageNet model을 supervised learning 한것 보다 더 좋은 성능을 보여주었다.

Natural supervision의 장점
---

1. 인터넷 등을 통해 기존 데이터보다 대규모의 데이터를 습득하기 쉽다.
2. 여러 representation간의 관계를 학습할 수 있다.

기존 supervision은 annotator가 image들을 하나하나 annotation해야 하는 반면 natural supervision은 인터넷을 통해 큰 데이터셋을 구축할 수 있다. CLIP에서는 4억개의 image-text pair를 구축했다. 

Training
---
![그림1](https://user-images.githubusercontent.com/46548053/110911131-a219f100-8355-11eb-9a49-0570e75707b9.png)  
기존 방법들은 predictive objective를 사용하여 word 하나하나를 맞추려고 했으나 하나의 image에는 여러 description이 가능해서 어려운 task였다. CLIP에서는 contrastive objective를 사용하고 text 전체를 하나로 보아 training하였다.
N개의 (image, text) pair가 있을 때 NxN cosine similarity를 구해서 real pair간의 simlilarity를 maximize하고 incorrect pair간의 similarity를 minimize하도록 training한다.

Experiment
---

CLIP의 task-learning capabilities를 측정하기 위해서 zero-shot transfer learning을 사용했다.  
![그림2](https://user-images.githubusercontent.com/46548053/110911162-ae05b300-8355-11eb-87a1-e1bd5b9a140e.png)  
기존 natural language supervision을 사용했던 Visual N-Grams과 성능을 비교하였을 시 큰폭의 성능향상이 있었다.   
![그림3](https://user-images.githubusercontent.com/46548053/110911168-af36e000-8355-11eb-9e4e-9bc8d8ac9bca.png)  
놀랍게도 Zero-shot learning을 한 CLIP은 fully supervised learning을 한 ResNet50보다 16개의 task에서 높은 성능을 보여주었다. CLIP이 안 좋은 성능을 보여준 task는 satellite image classification (EuroSAT and RESISC45), lymph node tumor detection과 같은 복잡하고 전문적인 지식이 필요한 task였다.  
![그림4](https://user-images.githubusercontent.com/46548053/110911172-b0680d00-8355-11eb-8d3f-a2c170d9bc63.png)  
기존 supervised learning, self supervised learning에서 SOTA성능을 보여준 모델들을 few-shot learning한 것과 CLIP을 Zero-shot learning, Few-shot learning한 것을 비교해봤는데 CLIP을 Zero-shot learning한 것이 기존 모델들을 few-shot learning 한 것보다 성능이 높았다. 흥미로운 부분은 4개의 sample을 few-shot learning 한 CLIP이 Zero-shot learning한 CLIP과 성능이 비슷하다는 것이다. Natural language supervision은 label간 연결되어 있는 반면 일반적인 supervision은 context가 없어 적은 데이터가 주어진 경우 탄탄한 가설을 생성하기 어려울 수 있다.
